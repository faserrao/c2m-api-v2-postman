#!/bin/bash
# =============================================================================
# C2M API Pipeline Test Suite
# =============================================================================
# Purpose: Comprehensive regression tests for the C2M API pipeline
# Author: Generated by Claude
# Date: September 27, 2025
# =============================================================================

# Exit on any error
set -euo pipefail

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Test results tracking
TESTS_PASSED=0
TESTS_FAILED=0
FAILED_TESTS=()

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[PASS]${NC} $1"
    ((TESTS_PASSED++))
}

log_error() {
    echo -e "${RED}[FAIL]${NC} $1"
    ((TESTS_FAILED++))
    FAILED_TESTS+=("$1")
}

log_warning() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_section() {
    echo ""
    echo -e "${BLUE}===================================================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}===================================================================${NC}"
}

# Check if a file exists
check_file_exists() {
    local file=$1
    local description=$2
    if [ -f "$file" ]; then
        log_success "$description exists: $file"
        return 0
    else
        log_error "$description missing: $file"
        return 1
    fi
}

# Check if a directory exists
check_dir_exists() {
    local dir=$1
    local description=$2
    if [ -d "$dir" ]; then
        log_success "$description exists: $dir"
        return 0
    else
        log_error "$description missing: $dir"
        return 1
    fi
}

# Check if a command exists
check_command() {
    local cmd=$1
    if command -v "$cmd" &> /dev/null; then
        log_success "Command available: $cmd"
        return 0
    else
        log_error "Command missing: $cmd"
        return 1
    fi
}

# Check JSON validity
check_json_valid() {
    local file=$1
    local description=$2
    if jq empty "$file" 2>/dev/null; then
        log_success "$description is valid JSON: $file"
        return 0
    else
        log_error "$description has invalid JSON: $file"
        return 1
    fi
}

# Check YAML validity
check_yaml_valid() {
    local file=$1
    local description=$2
    if yq eval '.' "$file" &>/dev/null; then
        log_success "$description is valid YAML: $file"
        return 0
    else
        log_error "$description has invalid YAML: $file"
        return 1
    fi
}

# Check if Makefile target exists
check_makefile_target() {
    local target=$1
    if grep -q "^${target}:" Makefile; then
        log_success "Makefile target exists: $target"
        return 0
    else
        log_error "Makefile target missing: $target"
        return 1
    fi
}

# Check collection structure
check_collection_structure() {
    local file=$1
    local description=$2
    
    # Check for required fields
    local has_info=$(jq -r '.info' "$file" 2>/dev/null)
    local has_item=$(jq -r '.item' "$file" 2>/dev/null)
    
    if [[ "$has_info" != "null" ]] && [[ "$has_item" != "null" ]]; then
        log_success "$description has valid Postman collection structure"
        
        # Additional checks
        local item_count=$(jq '.item | length' "$file" 2>/dev/null)
        log_info "  - Collection has $item_count items"
        
        # Check if collection is flattened (no nested folders)
        local has_nested=$(jq '[.item[] | select(has("item"))] | length' "$file" 2>/dev/null)
        if [[ "$has_nested" == "0" ]]; then
            log_info "  - Collection is flattened (no nested folders)"
        else
            log_info "  - Collection has nested folders"
        fi
        
        return 0
    else
        log_error "$description has invalid collection structure"
        return 1
    fi
}

# =============================================================================
# MODULE TESTS
# =============================================================================

test_prerequisites() {
    log_section "Testing Prerequisites"
    
    # Check required commands
    local commands=("make" "jq" "yq" "curl" "npx" "node" "python3")
    for cmd in "${commands[@]}"; do
        check_command "$cmd"
    done
    
    # Check environment files
    check_file_exists ".env" "Environment file"
    
    # Check if .env has required variables
    if [ -f ".env" ]; then
        if grep -q "POSTMAN_SERRAO_API_KEY" .env; then
            log_success "POSTMAN_SERRAO_API_KEY found in .env"
        else
            log_error "POSTMAN_SERRAO_API_KEY not found in .env"
        fi
    fi
}

test_directory_structure() {
    log_section "Testing Directory Structure"
    
    # Check required directories
    local dirs=(
        "data_dictionary"
        "openapi"
        "openapi/overlays"
        "postman"
        "postman/custom"
        "postman/generated"
        "scripts"
        "scripts/active"
        "scripts/utilities"
        "docs"
    )
    
    for dir in "${dirs[@]}"; do
        check_dir_exists "$dir" "Directory"
    done
}

test_source_files() {
    log_section "Testing Source Files"
    
    # Check EBNF data dictionary
    check_file_exists "data_dictionary/c2mapiv2-dd.ebnf" "EBNF data dictionary"
    
    # Check overlays
    check_file_exists "openapi/overlays/auth.tokens.yaml" "Auth overlay"
    check_yaml_valid "openapi/overlays/auth.tokens.yaml" "Auth overlay"
    
    # Check custom overrides
    if [ -f "postman/custom/overrides.json" ]; then
        check_json_valid "postman/custom/overrides.json" "Custom overrides"
    else
        log_warning "No custom overrides file found (optional)"
    fi
}

test_scripts() {
    log_section "Testing Required Scripts"
    
    # Check Python scripts
    local python_scripts=(
        "scripts/active/ebnf_to_openapi_dynamic_v3.py"
        "scripts/active/merge_openapi_overlays.py"
        "scripts/active/fix_collection_urls_v2.py"
    )
    
    for script in "${python_scripts[@]}"; do
        if check_file_exists "$script" "Python script"; then
            # Check if script is executable or can be run with python
            if python3 -m py_compile "$script" 2>/dev/null; then
                log_info "  - Script has valid Python syntax"
            else
                log_error "  - Script has Python syntax errors"
            fi
        fi
    done
    
    # Check Node.js scripts
    local node_scripts=(
        "scripts/active/add_tests.js"
        "scripts/active/validate_collection.js"
        "scripts/active/add_pre_request_script.js"
        "scripts/active/add_auth_examples.js"
    )
    
    for script in "${node_scripts[@]}"; do
        if check_file_exists "$script" "Node.js script"; then
            # Check if script has valid syntax
            if node -c "$script" 2>/dev/null; then
                log_info "  - Script has valid JavaScript syntax"
            else
                log_error "  - Script has JavaScript syntax errors"
            fi
        fi
    done
}

test_makefile_targets() {
    log_section "Testing Makefile Targets"
    
    # Critical targets that must exist
    local required_targets=(
        "postman-instance-build-and-test"
        "postman-cleanup-all"
        "generate-openapi-spec-from-ebnf-dd"
        "openapi-merge-overlays"
        "postman-import-openapi-spec"
        "postman-create-linked-collection"
        "postman-create-test-collection"
        "postman-create-mock-and-env"
        "prism-start"
        "postman-mock"
    )
    
    for target in "${required_targets[@]}"; do
        check_makefile_target "$target"
    done
}

# =============================================================================
# INTEGRATION TESTS
# =============================================================================

test_openapi_generation() {
    log_section "Testing OpenAPI Generation"
    
    # Run the generation
    log_info "Generating OpenAPI spec from EBNF..."
    if make generate-openapi-spec-from-ebnf-dd &>/dev/null; then
        log_success "OpenAPI generation completed"
        
        # Check outputs
        check_file_exists "openapi/c2mapiv2-openapi-spec-base.yaml" "Base OpenAPI spec"
        check_yaml_valid "openapi/c2mapiv2-openapi-spec-base.yaml" "Base OpenAPI spec"
        
        check_file_exists "openapi/c2mapiv2-openapi-spec-final.yaml" "Final OpenAPI spec"
        check_yaml_valid "openapi/c2mapiv2-openapi-spec-final.yaml" "Final OpenAPI spec"
        
        # Check if auth endpoints were merged
        if grep -q "/auth/tokens" "openapi/c2mapiv2-openapi-spec-final.yaml"; then
            log_success "Auth endpoints found in final spec"
        else
            log_error "Auth endpoints missing from final spec"
        fi
    else
        log_error "OpenAPI generation failed"
    fi
}

test_collection_generation() {
    log_section "Testing Collection Generation"
    
    log_info "Testing local collection generation..."
    
    # Generate raw collection
    if npx openapi-to-postmanv2 -s openapi/c2mapiv2-openapi-spec-final.yaml -o postman/generated/test-collection.json -p &>/dev/null; then
        log_success "Collection generation succeeded"
        check_json_valid "postman/generated/test-collection.json" "Generated collection"
        check_collection_structure "postman/generated/test-collection.json" "Generated collection"
        
        # Clean up test file
        rm -f postman/generated/test-collection.json
    else
        log_error "Collection generation failed"
    fi
}

test_collection_processing() {
    log_section "Testing Collection Processing Scripts"
    
    # Create a minimal test collection
    local test_collection="postman/generated/test-processing.json"
    cat > "$test_collection" <<'EOF'
{
  "info": {
    "name": "Test Collection",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "Test Request",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{baseUrl}}/test",
          "host": ["{{baseUrl}}"],
          "path": ["test"]
        }
      }
    }
  ]
}
EOF
    
    # Test adding pre-request script
    if [ -f "scripts/active/add_pre_request_script.js" ] && [ -f "postman/scripts/jwt-pre-request.js" ]; then
        log_info "Testing pre-request script addition..."
        if node scripts/active/add_pre_request_script.js "$test_collection" postman/scripts/jwt-pre-request.js "$test_collection" &>/dev/null; then
            log_success "Pre-request script addition succeeded"
            
            # Check if script was added
            if jq -e '.item[0].event[0].script.exec' "$test_collection" &>/dev/null; then
                log_info "  - Pre-request script found in collection"
            fi
        else
            log_warning "Pre-request script addition failed (may be optional)"
        fi
    fi
    
    # Clean up
    rm -f "$test_collection"
}

# =============================================================================
# END-TO-END TESTS
# =============================================================================

test_full_pipeline_dry_run() {
    log_section "Testing Full Pipeline (Dry Run)"
    
    # Check if we can run the main targets without errors
    log_info "Checking if main pipeline targets can be executed..."
    
    # Test cleanup
    if make -n postman-cleanup-all &>/dev/null; then
        log_success "Cleanup target is valid"
    else
        log_error "Cleanup target has errors"
    fi
    
    # Test main build
    if make -n postman-instance-build-and-test &>/dev/null; then
        log_success "Main build target is valid"
    else
        log_error "Main build target has errors"
    fi
}

# =============================================================================
# POSTMAN API TESTS
# =============================================================================

test_postman_connectivity() {
    log_section "Testing Postman API Connectivity"
    
    # Source .env to get API key
    if [ -f ".env" ]; then
        source .env
        
        if [ -n "$POSTMAN_SERRAO_API_KEY" ]; then
            log_info "Testing Postman API connection..."
            
            # Test API key validity
            response=$(curl -s -o /dev/null -w "%{http_code}" \
                -H "X-Api-Key: $POSTMAN_SERRAO_API_KEY" \
                "https://api.getpostman.com/me")
            
            if [ "$response" = "200" ]; then
                log_success "Postman API key is valid"
            else
                log_error "Postman API key is invalid (HTTP $response)"
            fi
            
            # Test workspace access
            if [ -n "${POSTMAN_WS:-}" ]; then
                response=$(curl -s -o /dev/null -w "%{http_code}" \
                    -H "X-Api-Key: $POSTMAN_SERRAO_API_KEY" \
                    "https://api.getpostman.com/workspaces/$POSTMAN_WS")
                
                if [ "$response" = "200" ]; then
                    log_success "Workspace $POSTMAN_WS is accessible"
                else
                    log_error "Workspace $POSTMAN_WS is not accessible (HTTP $response)"
                fi
            fi
        else
            log_error "POSTMAN_SERRAO_API_KEY not set in .env"
        fi
    fi
}

# =============================================================================
# GITHUB ACTIONS TESTS
# =============================================================================

test_github_actions() {
    log_section "Testing GitHub Actions Configuration"
    
    # Check for workflow files
    check_file_exists ".github/workflows/api-ci-cd.yml" "Main CI/CD workflow"
    check_file_exists ".github/workflows/pr-drift-check.yml" "PR drift check workflow"
    
    # Check for CI-specific targets
    local ci_targets=(
        "openapi-build"
        "postman-collection-build"
        "docs"
        "lint"
        "diff"
    )
    
    log_info "Checking CI-specific Makefile targets..."
    for target in "${ci_targets[@]}"; do
        check_makefile_target "$target"
    done
}

# =============================================================================
# REPORT GENERATION
# =============================================================================

generate_report() {
    log_section "Test Summary"
    
    local total=$((TESTS_PASSED + TESTS_FAILED))
    echo -e "Total tests: $total"
    echo -e "Passed: ${GREEN}$TESTS_PASSED${NC}"
    echo -e "Failed: ${RED}$TESTS_FAILED${NC}"
    
    if [ ${#FAILED_TESTS[@]} -gt 0 ]; then
        echo -e "\nFailed tests:"
        for test in "${FAILED_TESTS[@]}"; do
            echo -e "  ${RED}‚úó${NC} $test"
        done
    fi
    
    echo ""
    if [ $TESTS_FAILED -eq 0 ]; then
        echo -e "${GREEN}‚úÖ All tests passed!${NC}"
        return 0
    else
        echo -e "${RED}‚ùå Some tests failed${NC}"
        return 1
    fi
}

# =============================================================================
# MAIN EXECUTION
# =============================================================================

main() {
    echo "üß™ C2M API Pipeline Test Suite"
    echo "üìÖ $(date)"
    echo ""
    
    # Run all test categories
    test_prerequisites
    test_directory_structure
    test_source_files
    test_scripts
    test_makefile_targets
    test_openapi_generation
    test_collection_generation
    test_collection_processing
    test_full_pipeline_dry_run
    test_postman_connectivity
    test_github_actions
    
    # Generate final report
    generate_report
}

# Parse command line arguments
if [ $# -gt 0 ]; then
    case "$1" in
        --help|-h)
            echo "Usage: $0 [options]"
            echo "Options:"
            echo "  --help, -h     Show this help message"
            echo "  --quick        Run only essential tests"
            echo "  --module       Run only module tests"
            echo "  --integration  Run only integration tests"
            echo "  --github       Run only GitHub Actions tests"
            exit 0
            ;;
        --quick)
            test_prerequisites
            test_makefile_targets
            generate_report
            ;;
        --module)
            test_prerequisites
            test_directory_structure
            test_source_files
            test_scripts
            test_makefile_targets
            generate_report
            ;;
        --integration)
            test_openapi_generation
            test_collection_generation
            test_collection_processing
            test_full_pipeline_dry_run
            generate_report
            ;;
        --github)
            test_github_actions
            generate_report
            ;;
        *)
            echo "Unknown option: $1"
            echo "Run with --help for usage information"
            exit 1
            ;;
    esac
else
    # Run all tests
    main
fi